# Define train and test sets ----------------------------------------------
```{r}
set.seed(123) 
trainIndexover <- createDataPartition(data_customer_segmentation$target_categorical, p = .8, 
                                  list = FALSE, 
                                  times = 1)
data_testover <- data_customer_segmentation[-trainIndexover, ]
data_trainover <- data_customer_segmentation[trainIndexover, ]
```

# Applyin oversampling
```{r}
table(data_trainover$target_categorical)
# Calcular o número de instâncias na classe majoritária
num_majoritaria <- sum(data_trainover$target_categorical == "non.buyer")

# Realizar oversampling
data_trainover <- ovun.sample(target_categorical ~ ., 
                                data = data_trainover, 
                                method = "over", 
                                N = 38000,  # Um valor grande suficiente
                                seed = 123)$data

# Verificar a distribuição da classe após oversampling
table(data_trainover$target_categorical)

```

# Classification class distribution in full dataset
```{r}
freq_table <- table(data_trainover$target_categorical)
cbind(Frequency = freq_table,
      Proportion = round(prop.table(freq_table), 3))
```



# metric roc
```{r}
metric <- "ROC"
control <- trainControl(method = "repeatedcv",
                        number = 5, # number of folds
                        repeats = 1, # number of repetitions
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        savePredictions = TRUE,
                        verboseIter = TRUE)
```

# Learn Decision Tree (rpart)
```{r}
seed <- 123
set.seed(seed)
model_DT <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster + credit + boleto + debit + voucher ,
                  data = data_trainover,
                  method = "rpart", # Decision Trees, with CART algorithm
                  metric = metric,
                  trControl = control)
gc()
```

# Learn Random Forest (rf)
```{r}
set.seed(seed)
model_RF <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster + credit + boleto + debit + voucher ,
                  data = data_trainover,
                  method = "rf",  # Random Forests
                  metric = metric,
                  trControl = control)
gc()
```

# Check models, metrics and specific tunning parameters
# cp: Complexity parameter
# mtry: Number of randomly drawn candidate variables
```{r}
print(model_DT)

print(model_RF)
```

# Calculate and compare models' performance
# Note:
#      We'll use function 'resamples()' for visualization
#      of the resampling results
#      These calculations are based on resampling the TRAIN set
```{r}
fit_models <- list(Decision_tree = model_DT, 
                   Random_forest = model_RF)
results <- resamples(fit_models) 
dotplot(results)
summary(results)
```

# Check Complexity Parameter (cp) used for trained model
```{r}
print(model_DT$results$cp)
```

# Check mtry parameter used for trained model
# getModelInfo(model_RF)$rf$parameters
```{r}
print(model_RF$results$mtry)
```

#Plot ROC vs used parameters for model
```{r}
plot(model_RF, main = "Random Forest")
```

# Improve the Random Forest model -----------------------------------------
# To improve the RF model, we fine tune the mtry parameter
```{r}
tune_grid <- expand.grid(mtry = c(2, 3, 4))

set.seed(seed)
model_RF_tuned <- train(target_categorical ~ monetary_cluster + frequency_cluster + recency_cluster + credit + boleto + debit + voucher ,
                       data = data_trainover,
                       method = "rf",
                       metric = metric,
                       trControl = control,
                       tuneGrid = tune_grid)
```

# Inspect model
```{r}
print(model_RF_tuned)
```

# Plot ROC vs used parameters
```{r}
plot(model_RF_tuned,
     main = "Random Forest - tuned for mtry parameter")
```

# Compare all models ------------------------------------------------------
# Calculate and compare models' performance
```{r}
it_models <- list(Decision_tree = model_DT, 
                   Random_forest = model_RF,
                   Random_forest_tuned = model_RF_tuned)
results <- resamples(fit_models)
dotplot(results)
summary(results)
```

# Model Validation --------------------------------------------------------
# To validate the model, we'll make predictions from the TEST set
# Estimate performance of Random Forest on the TEST dataset
```{r}
predictions_prob <- predict(model_RF_tuned, data_testover, type = "prob")
head(predictions_prob)
predictions_raw <- predict(model_RF_tuned, data_testover, type = "raw")
head(cbind(predictions_prob, predictions_raw))
```

# Confusion Matrix
```{r}
confusionMatrix(data = predictions_raw,
                reference = data_testover$target_categorical)
                # Tip: use the following argument for more metrics
                #   mode = "everything"
                # Note that:
                #   Precison = Pos Pred Value
                #   Recall = Sensitivity
```